{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7134f8a6-8fd4-4d06-ada4-78dd64a78250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def posterior(Phi, t, alpha, beta, return_inverse=False):\n",
    "    \"\"\"Computes mean and covariance matrix of the posterior distribution.\"\"\"\n",
    "    S_N_inv = alpha * np.eye(Phi.shape[1]) + beta * Phi.T.dot(Phi)\n",
    "    S_N = np.linalg.inv(S_N_inv)\n",
    "    m_N = beta * S_N.dot(Phi.T).dot(t)\n",
    "\n",
    "    if return_inverse:\n",
    "        return m_N, S_N, S_N_inv\n",
    "    else:\n",
    "        return m_N, S_N\n",
    "\n",
    "\n",
    "def posterior_predictive(Phi_test, m_N, S_N, beta):\n",
    "    \"\"\"Computes mean and variances of the posterior predictive distribution.\"\"\"\n",
    "    y = Phi_test.dot(m_N)\n",
    "    # Only compute variances (diagonal elements of covariance matrix)\n",
    "    y_var = 1 / beta + np.sum(Phi_test.dot(S_N) * Phi_test, axis=1)\n",
    "    \n",
    "    return y, y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f78ea5-eaac-45d4-b7f5-bd915f8afb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST DATASET\n",
    "\n",
    "f_w0 = -0.3\n",
    "f_w1 =  0.5\n",
    "\n",
    "\n",
    "def f(X, noise_variance):\n",
    "    '''Linear function plus noise'''\n",
    "    return f_w0 + f_w1 * X + noise(X.shape, noise_variance)\n",
    "\n",
    "\n",
    "def g(X, noise_variance):\n",
    "    '''Sinusoidial function plus noise'''\n",
    "    return 0.5 + np.sin(2 * np.pi * X) + noise(X.shape, noise_variance)\n",
    "\n",
    "\n",
    "def noise(size, variance):\n",
    "    return np.random.normal(scale=np.sqrt(variance), size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af654bc-0121-4e60-84e3-d72f35f6aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_basis_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def gaussian_basis_function(x, mu, sigma=0.1):\n",
    "    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)\n",
    "\n",
    "\n",
    "def polynomial_basis_function(x, power):\n",
    "    return x ** power\n",
    "\n",
    "\n",
    "def expand(x, bf, bf_args=None):\n",
    "    if bf_args is None:\n",
    "        return np.concatenate([np.ones(x.shape), bf(x)], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([np.ones(x.shape)] + [bf(x, bf_arg) for bf_arg in bf_args], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc14c69-4d44-4587-8403-ecdc466cc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011a420a-3590-4f9c-a1e6-4bb331858aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bayesian_linear_regression_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesian_linear_regression_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bayesian_linear_regression_util'"
     ]
    }
   ],
   "source": [
    "from bayesian_linear_regression_util import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Training dataset sizes\n",
    "N_list = [1, 3, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1) * 2 - 1\n",
    "\n",
    "# Training target values\n",
    "t = f(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = f(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, identity_basis_function)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, identity_basis_function)\n",
    "    \n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 3, i * 3 + 1)\n",
    "    plot_posterior(m_N, S_N, f_w0, f_w1)\n",
    "    plt.title(f'Posterior density (N = {N})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 3)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61605ec-3f22-4a00-ba73-13f4bbe00b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kuhak\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\kuhak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kuhak\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\kuhak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\kuhak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6455da88-22cf-47cb-98fa-cc2fdb6a2118",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posterior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m Phi_N \u001b[38;5;241m=\u001b[39m expand(X_N, bf\u001b[38;5;241m=\u001b[39mgaussian_basis_function, bf_args\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Mean and covariance matrix of posterior\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m m_N, S_N \u001b[38;5;241m=\u001b[39m \u001b[43mposterior\u001b[49m(Phi_N, t_N, alpha, beta)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Mean and variances of posterior predictive \u001b[39;00m\n\u001b[0;32m     35\u001b[0m y, y_var \u001b[38;5;241m=\u001b[39m posterior_predictive(Phi_test, m_N, S_N, beta)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'posterior' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_list = [3, 8, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1)\n",
    "\n",
    "# Training target values\n",
    "t = g(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = g(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 1)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020388a-351e-4bd0-b9ed-79236d13efeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
