{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9b7cb0-4083-4b17-924e-761f0b6ee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca99eaac-6489-4221-ade3-a16e06d3d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('corpus.txt', 'r', errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c8b681-5b4a-44d7-92b3-0b516614717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2118d8-2613-476d-8e16-7ae06a719bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= raw_data.lower()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560e11e5-bbfd-40bd-a108-2f5c625ea98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nuniversity\\nfrom wikipedia, the free encyclopedia\\njump to navigation\\njump to search\\nfor other uses, see university (disambiguation).\\nthe alma mater, meaning \"nourishing mother\" in latin, is one of the'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f8d3b5-400f-4779-81c3-6ec8923dee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_data)\n",
    "word_tokens = nltk.word_tokenize(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d1738c-eab9-43b8-b41b-d3836bb1e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['university', 'from', 'wikipedia', ',', 'the']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b331faa-fe4b-49f2-ba60-e1c248561e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nuniversity\\nfrom wikipedia, the free encyclopedia\\njump to navigation\\njump to search\\nfor other uses, see university (disambiguation).',\n",
       " 'the alma mater, meaning \"nourishing mother\" in latin, is one of the most enduring symbols of the university.',\n",
       " \"the phrase was first used to describe the university of bologna, founded in 1088.\\n\\na university (from latin universitas√¢\\xa0'a whole') is an institution of higher (or tertiary) education and research which awards academic degrees in several academic disciplines.\",\n",
       " 'universities typically offer both undergraduate and postgraduate programs.',\n",
       " 'in the united states, the designation is reserved for colleges that have a graduate school.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272ce96f-c3e8-4d9f-ad88-bb0db26adb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def Lemtokens(tokens):\n",
    "    return [lemmer.lemmatize(tokens) for token in tokens]\n",
    "\n",
    "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def Lemnormalize(text):\n",
    "    return Lemtokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12592ba7-9002-49e8-9c27-09927fd2f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_input = ('Hi','Hiya','How do you do','Hi there','Hello','Whats going on')\n",
    "\n",
    "greet_responses = ('Hi','Hiya','How do you do','Hi there','Hello','Whats going on')\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_input:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ae5390-586e-49b2-a287-3f471dade238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035714a7-9122-4097-b106-5372e2e917a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    r1_response = ''\n",
    "    TfidfVec= TfidfVectorizer(tokenizer=Lemnormalize, stop_words= 'english')\n",
    "    tfidf=TfidfVec.fit_transform(sentence_tokens)\n",
    "    \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx= vals.argsort()[0][-2]\n",
    "    flat=vals.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    req_tfid=flat[-2]\n",
    "    \n",
    "    if(req_tfid==0):\n",
    "        r1_response= r1_response+ \"Sorry, I could not understand you\"\n",
    "        return r1_response\n",
    "    else:\n",
    "        r1_response=r1_response+sentence_tokens[idx]\n",
    "        return r1_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e7ea1e4-6b87-4695-9db8-1fc1d652001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am CouncilBot, a self learning bot, please greet me and talk to me about universities\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " University\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CouncilBot: "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             final_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(word_tokens))\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCouncilBot: \u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_response\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m             sentence_tokens\u001b[38;5;241m.\u001b[39mremove(user_response)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mresponse\u001b[1;34m(user_response)\u001b[0m\n\u001b[0;32m      2\u001b[0m r1_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m TfidfVec\u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mLemnormalize, stop_words\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m tfidf\u001b[38;5;241m=\u001b[39m\u001b[43mTfidfVec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m vals \u001b[38;5;241m=\u001b[39m cosine_similarity(tfidf[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], tfidf)\n\u001b[0;32m      7\u001b[0m idx\u001b[38;5;241m=\u001b[39m vals\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mLemnormalize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLemnormalize\u001b[39m(text):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLemtokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_punc_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mLemtokens\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLemtokens\u001b[39m(tokens):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [lemmer\u001b[38;5;241m.\u001b[39mlemmatize(tokens) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLemtokens\u001b[39m(tokens):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mlemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2032\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;66;03m# 0. Check the exception lists\u001b[39;00m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_exceptions:\n\u001b[1;32m-> 2032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexceptions\u001b[49m:\n\u001b[0;32m   2033\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m filter_forms([form] \u001b[38;5;241m+\u001b[39m exceptions[form])\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "\n",
    "print('Hello, I am CouncilBot, a self learning bot, please greet me and talk to me about universities')\n",
    "\n",
    "while(flag==True):\n",
    "    user_response=input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks'):\n",
    "            flag=False\n",
    "            print('CouncilBot: You are welcome')\n",
    "        else:\n",
    "            if(greeting(user_response)!= None):\n",
    "                print('CouncilBot: '+greeting(user_response))\n",
    "            else:\n",
    "                sentence_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print('CouncilBot: ', end='')\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print('CouncilBot: Goodbye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aec931-6d35-44a4-aed2-d22ce264ff94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
